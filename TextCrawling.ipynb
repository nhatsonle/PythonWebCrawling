{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless=new')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "driver = webdriver.Chrome(\n",
    "  options=chrome_options\n",
    ")\n",
    "\n",
    "root_dir = './vn_news corpus'\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "n_pages = 10\n",
    "article_id = 0\n",
    "\n",
    "for page_idx in range(n_pages):\n",
    "  main_url = f'https://vietnamnet.vn/thoi-su-page{page_idx}'\n",
    "  driver.get(main_url)\n",
    "  \n",
    "  news_lst_xpath = '//div[@class=\"topStory-15nd\"]/div/div[1]/a' #Lay tat ca nhung the div co class = \"topStory-15nd\"\n",
    "  news_tags = driver.find_elements(\n",
    "    By.XPATH,\n",
    "    news_lst_xpath\n",
    "  )\n",
    "  \n",
    "  news_page_urls = [\n",
    "    news_tag.get_attribute('href') \\\n",
    "    for news_tag in news_tags\n",
    "  ]\n",
    "  \n",
    "  for news_page_url in news_page_urls:\n",
    "    driver.get(news_page_url)\n",
    "    time.sleep(1) #Dừng 1 khoảng thời gian để tránh bị server identify là bot\n",
    "    \n",
    "    main_content_xpath = '//div[@class=\"content-detail\"]'\n",
    "    try:\n",
    "      main_content_tag = driver.find_element(\n",
    "          By.XPATH,\n",
    "          main_content_xpath\n",
    "      )\n",
    "    except:\n",
    "      continue\n",
    "    \n",
    "    video_content_xpath = '//div[@class=\"video-detail\"]'\n",
    "    try:\n",
    "      video_content_tag = main_content_tag.find_element(\n",
    "        By.XPATH,\n",
    "        video_content_xpath\n",
    "      )\n",
    "      continue\n",
    "    except:\n",
    "      pass\n",
    "    \n",
    "    title = main_content_tag.find_element(\n",
    "      By.TAG_NAME,\n",
    "      'h1'\n",
    "    ).text.strip()\n",
    "    \n",
    "    abstract = main_content_tag.find_element(\n",
    "      By.TAG_NAME,\n",
    "      'h2'\n",
    "    ).text.strip()\n",
    "    \n",
    "    try:\n",
    "      author_xpath = '//span[@class=\"name\"]'\n",
    "      author = main_content_tag.find_element(\n",
    "        By.XPATH,\n",
    "        author_xpath\n",
    "      ).text.strip()\n",
    "    except:\n",
    "      author = ''\n",
    "      \n",
    "    paragraphs_xpath = '//div[@class=\"maincontent main-content\"]/p'\n",
    "    paragraphs_tags = main_content_tag.find_element(\n",
    "      By.XPATH,\n",
    "      paragraphs_xpath\n",
    "    )\n",
    "    \n",
    "    paragraphs_lst = [\n",
    "      paragraphs_tag.text.strip()  \n",
    "        for paragraphs_tag in paragraphs_tags\n",
    "    ]\n",
    "    paragraphs = ' '.join(paragraphs_lst)\n",
    "    final_content_lst = [title, abstract, paragraphs, author]\n",
    "    final_content = '\\n\\n'.join(final_content_lst)\n",
    "        \n",
    "    article_filename = f'article_{article_id:05d}.txt'\n",
    "    article_savepath = os.path.join(\n",
    "      root_dir,\n",
    "      article_filename\n",
    "    ) \n",
    "    article_id += 1\n",
    "    with open(article_savepath, 'w') as f:\n",
    "      f.write(final_content)\n",
    "\n",
    "    driver.back()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
